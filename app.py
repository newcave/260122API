import os
import re
from dataclasses import dataclass
from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urljoin

import requests
import streamlit as st
from bs4 import BeautifulSoup
import pdfplumber
from pypdf import PdfReader
from openai import OpenAI

# ======================================================
# App Config
# ======================================================
APP_TITLE = "ALIO ì—°êµ¬ë³´ê³ ì„œ ìš”ì•½ ì—ì´ì „íŠ¸ (K-water í‘œì¤€ A / Bì•ˆ ìë™ëŒ€ì‘)"
BASE = "https://alio.go.kr"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Referer": BASE,
    "Accept": "application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ìˆ˜ìì›ê³µì‚¬(K-water) ë° ê³µê³µê¸°ê´€ ì—°êµ¬ë³´ê³ ì„œë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ì •ì±…Â·ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë³´ê³ ì„œë¥¼ 'K-water ì—°êµ¬ë³´ê³ ì„œ í‘œì¤€ A ìš”ì•½ í˜•ì‹'ì— ë§ì¶° ìš”ì•½í•˜ì‹­ì‹œì˜¤.

[ì¶œë ¥ í˜•ì‹ â€” ë°˜ë“œì‹œ ì¤€ìˆ˜]

## 1. ì—°êµ¬ ë°°ê²½ ë° í•„ìš”ì„±
## 2. ì—°êµ¬ ëª©ì 
## 3. ì—°êµ¬ ë²”ìœ„ ë° ë°©ë²•
## 4. ì£¼ìš” ì—°êµ¬ ê²°ê³¼
## 5. ì •ì±…ì Â·ì‹¤ë¬´ì  ì‹œì‚¬ì 
## 6. ê²°ë¡  ë° í–¥í›„ ê³¼ì œ
"""

# ======================================================
# Models
# ======================================================
@dataclass
class ListProbeResult:
    endpoint: str
    method: str
    params: Dict[str, Any]
    list_key: str
    total_key: Optional[str]

@dataclass
class ReportCandidate:
    title: str
    org: str
    date: str
    detail_url: Optional[str]
    raw: Dict[str, Any]  # keep original for debugging / id extraction

# ======================================================
# Utils: HTTP
# ======================================================
def safe_get(url: str, params: Optional[Dict[str, Any]] = None, timeout: int = 15) -> requests.Response:
    r = requests.get(url, params=params, headers=HEADERS, timeout=timeout, allow_redirects=True)
    r.raise_for_status()
    return r

def safe_post(url: str, json_body: Optional[Dict[str, Any]] = None, timeout: int = 15) -> requests.Response:
    r = requests.post(url, json=json_body, headers=HEADERS, timeout=timeout, allow_redirects=True)
    r.raise_for_status()
    return r

def is_json_response(resp: requests.Response) -> bool:
    ct = (resp.headers.get("Content-Type") or "").lower()
    if "application/json" in ct:
        return True
    # Some servers mislabel; attempt json parsing
    try:
        resp.json()
        return True
    except Exception:
        return False

# ======================================================
# 1) Bì•ˆ í•µì‹¬: ëª©ë¡ API ìë™ íƒìƒ‰(í”„ë¡œë¹™)
# ======================================================
LIST_ENDPOINT_CANDIDATES = [
    # ê°€ì¥ í”íˆ ì“°ì´ëŠ” íŒ¨í„´(ê¸°ê´€/ìœ í˜•/í˜ì´ì§•)
    (f"{BASE}/iris/api/report/list.json", "GET"),
    (f"{BASE}/iris/api/report/list", "GET"),
    # ë‹¤ë¥¸ ì‹œìŠ¤í…œì—ì„œ í”í•œ ì´ë¦„ í›„ë³´ë“¤(ê¸°ê´€ë³„/ê³µì‹œë³„ ì»¤ìŠ¤í…€ ì¼€ì´ìŠ¤ ëŒ€ë¹„)
    (f"{BASE}/iris/api/report/itemReportList.json", "GET"),
    (f"{BASE}/iris/api/report/itemReportListSusi.json", "GET"),
]

PARAM_SETS = [
    # ì¼€ì´ìŠ¤ A: ì§ˆë¬¸ì—ì„œ ì“°ì‹  íŒŒë¼ë¯¸í„°ëª…
    {"apbaId": None, "reportFormRootNo": None, "pageIndex": 1, "pageSize": 30},
    # ì¼€ì´ìŠ¤ B: page/pageSize
    {"apbaId": None, "reportFormRootNo": None, "page": 1, "pageSize": 30},
    # ì¼€ì´ìŠ¤ C: size/curPage ê°™ì€ ë³€í˜•
    {"apbaId": None, "reportFormRootNo": None, "curPage": 1, "pageSize": 30},
    {"apbaId": None, "reportFormRootNo": None, "pageNo": 1, "pageCnt": 30},
]

POSSIBLE_LIST_KEYS = ["list", "data", "result", "rows", "items"]
POSSIBLE_TOTAL_KEYS = ["totalCount", "total", "records", "count", "totCnt"]

def extract_list_from_json(data: Any) -> Tuple[Optional[List[Any]], Optional[str]]:
    """
    ë°˜í™˜ JSONì—ì„œ ë¦¬ìŠ¤íŠ¸ í›„ë³´ í‚¤ë¥¼ ì°¾ì•„ ì‹¤ì œ listë¥¼ ë½‘ì•„ëƒ„
    """
    if isinstance(data, list):
        return data, "(root_list)"
    if not isinstance(data, dict):
        return None, None

    for k in POSSIBLE_LIST_KEYS:
        v = data.get(k)
        if isinstance(v, list):
            return v, k

    # 2-depth íƒìƒ‰ (ì˜ˆ: {"result": {"list":[...]}})
    for k in data.keys():
        v = data.get(k)
        if isinstance(v, dict):
            for kk in POSSIBLE_LIST_KEYS:
                vv = v.get(kk)
                if isinstance(vv, list):
                    return vv, f"{k}.{kk}"

    return None, None

def probe_report_list_api(apba_id: str, report_root: str) -> ListProbeResult:
    """
    ì—¬ëŸ¬ í›„ë³´ endpoint/paramsë¥¼ ì‹œë„í•´ì„œ 'ì‹¤ì œë¡œ ë™ì‘í•˜ëŠ”' ëª©ë¡ API ì¡°í•©ì„ ì°¾ì•„ ë°˜í™˜
    """
    last_err = None
    for endpoint, method in LIST_ENDPOINT_CANDIDATES:
        for base_params in PARAM_SETS:
            params = dict(base_params)
            params["apbaId"] = apba_id
            params["reportFormRootNo"] = report_root

            try:
                if method == "GET":
                    resp = safe_get(endpoint, params=params)
                else:
                    resp = safe_post(endpoint, json_body=params)

                if not is_json_response(resp):
                    continue

                data = resp.json()
                items, list_key = extract_list_from_json(data)
                if items is None or len(items) == 0:
                    # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ë„ totalì´ ìˆìœ¼ë©´ ì„±ê³µì¼ ìˆ˜ ìˆì§€ë§Œ,
                    # ì—¬ê¸°ì„œëŠ” "ì‹¤ì œë¡œ ë¦¬ìŠ¤íŠ¸ í‚¤ë¥¼ ì°¾ì•˜ëŠ”ê°€"ë¥¼ ìš°ì„ 
                    continue

                # total í‚¤ ì¶”ì •
                total_key = None
                if isinstance(data, dict):
                    for tk in POSSIBLE_TOTAL_KEYS:
                        if tk in data:
                            total_key = tk
                            break
                    # 2-depth total
                    if total_key is None:
                        for k in data.keys():
                            if isinstance(data.get(k), dict):
                                for tk in POSSIBLE_TOTAL_KEYS:
                                    if tk in data[k]:
                                        total_key = f"{k}.{tk}"
                                        break

                return ListProbeResult(
                    endpoint=endpoint,
                    method=method,
                    params=params,
                    list_key=list_key,
                    total_key=total_key,
                )
            except Exception as e:
                last_err = e
                continue

    raise RuntimeError(f"ëª©ë¡ API ìë™ íƒìƒ‰ ì‹¤íŒ¨ (ë§ˆì§€ë§‰ ì—ëŸ¬: {last_err})")

def get_list_with_probe(probe: ListProbeResult, page: int = 1, page_size: int = 30) -> Dict[str, Any]:
    """
    íƒìƒ‰ëœ probe ì¡°í•©ìœ¼ë¡œ ì‹¤ì œ ëª©ë¡ì„ ê°€ì ¸ì˜¨ë‹¤ (í˜ì´ì§€ ë°˜ì˜)
    """
    params = dict(probe.params)
    # í˜ì´ì§€ í‚¤ ìë™ ë°˜ì˜
    if "pageIndex" in params:
        params["pageIndex"] = page
    elif "page" in params:
        params["page"] = page
    elif "curPage" in params:
        params["curPage"] = page
    elif "pageNo" in params:
        params["pageNo"] = page

    if "pageSize" in params:
        params["pageSize"] = page_size
    elif "pageCnt" in params:
        params["pageCnt"] = page_size

    if probe.method == "GET":
        resp = safe_get(probe.endpoint, params=params)
    else:
        resp = safe_post(probe.endpoint, json_body=params)

    if not is_json_response(resp):
        raise RuntimeError("ëª©ë¡ API ì‘ë‹µì´ JSONì´ ì•„ë‹™ë‹ˆë‹¤.")
    return resp.json()

def normalize_candidates(list_json: Any) -> List[ReportCandidate]:
    """
    ëª©ë¡ JSONì—ì„œ 'ì œëª©/ê¸°ê´€/ì¼ì/ìƒì„¸ë§í¬'ë¥¼ ìµœëŒ€í•œ ë³µì›
    """
    items, _ = extract_list_from_json(list_json)
    if items is None:
        return []

    candidates: List[ReportCandidate] = []
    for it in items:
        if not isinstance(it, dict):
            continue

        title = (
            it.get("reportTitle")
            or it.get("rtitle")
            or it.get("title")
            or it.get("sj")
            or "(ì œëª©ì—†ìŒ)"
        )
        org = (
            it.get("apbaNm")
            or it.get("orgNm")
            or it.get("instNm")
            or it.get("org")
            or ""
        )
        date = (
            it.get("regDate")
            or it.get("regDt")
            or it.get("pubDate")
            or it.get("publishDate")
            or it.get("ymd")
            or ""
        )

        # ìƒì„¸ URL í›„ë³´
        detail_url = (
            it.get("detailUrl")
            or it.get("detailURL")
            or it.get("linkUrl")
            or it.get("url")
            or None
        )
        # ì¼ë¶€ëŠ” ìƒëŒ€ê²½ë¡œì¼ ìˆ˜ ìˆì–´ join ì²˜ë¦¬
        if isinstance(detail_url, str) and detail_url.startswith("/"):
            detail_url = urljoin(BASE, detail_url)

        candidates.append(
            ReportCandidate(
                title=str(title).strip(),
                org=str(org).strip(),
                date=str(date).strip(),
                detail_url=detail_url,
                raw=it,
            )
        )

    return candidates

# ======================================================
# 2) PDF ë§í¬ ì¶”ì¶œ: (A) JSON ìƒì„¸ API í”„ë¡œë¹™ â†’ ì‹¤íŒ¨ì‹œ (B) HTML íŒŒì‹±
# ======================================================
DETAIL_ENDPOINT_CANDIDATES = [
    # í”í•œ ìƒì„¸ íŒ¨í„´ í›„ë³´ë“¤
    f"{BASE}/iris/api/report/detail.json",
    f"{BASE}/iris/api/report/detail",
    f"{BASE}/iris/api/report/view.json",
    f"{BASE}/iris/api/report/view",
]

DETAIL_ID_KEYS = ["reportNo", "reportSn", "rptNo", "id", "seq", "reportId", "reportFormNo", "reportRootNo"]

def guess_id_fields(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    ëª©ë¡ itemì—ì„œ ìƒì„¸ ì¡°íšŒì— ì“¸ë§Œí•œ ID í•„ë“œë¥¼ ìµœëŒ€í•œ ì°¾ì•„ dictë¡œ ë°˜í™˜
    """
    found = {}
    for k in DETAIL_ID_KEYS:
        if k in item and item[k] not in (None, "", 0):
            found[k] = item[k]
    return found

def extract_pdf_from_detail_json(detail_json: Any) -> Optional[str]:
    """
    ìƒì„¸ JSONì—ì„œ PDF ë‹¤ìš´ë¡œë“œ URLì„ ì°¾ëŠ”ë‹¤.
    """
    if not isinstance(detail_json, dict):
        return None

    # í”í•œ ì²¨ë¶€ êµ¬ì¡°ë“¤
    for key in ["attachFiles", "files", "fileList", "attachments"]:
        v = detail_json.get(key)
        if isinstance(v, list):
            for f in v:
                if not isinstance(f, dict):
                    continue
                ext = (f.get("fileExt") or f.get("ext") or "").lower()
                name = (f.get("fileNm") or f.get("name") or f.get("fileName") or "").lower()
                url = f.get("downloadUrl") or f.get("downUrl") or f.get("url")

                if url and isinstance(url, str):
                    if ext == "pdf" or name.endswith(".pdf") or ".pdf" in url.lower():
                        return urljoin(BASE, url) if url.startswith("/") else url

    # ì–´ë–¤ ê²½ìš°ëŠ” ë‹¨ì¼ pdfUrl í•„ë“œê°€ ìˆìŒ
    for key in ["pdfUrl", "pdfURL", "downloadUrl"]:
        v = detail_json.get(key)
        if isinstance(v, str) and v:
            if ".pdf" in v.lower() or "filedown" in v.lower() or "download" in v.lower():
                return urljoin(BASE, v) if v.startswith("/") else v

    return None

def probe_detail_api_for_pdf(item: Dict[str, Any]) -> Optional[str]:
    """
    ìƒì„¸ APIë¥¼ ì—¬ëŸ¬ í›„ë³´ ì—”ë“œí¬ì¸íŠ¸/íŒŒë¼ë¯¸í„°ë¡œ ì‹œë„í•´ì„œ PDF URLì„ ì–»ëŠ”ë‹¤.
    """
    id_fields = guess_id_fields(item)
    if not id_fields:
        return None

    # ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ë§Œë“ ë‹¤:
    # 1) reportNo ìš°ì„ , 2) id/seq ë“± ëŒ€ì²´
    param_candidates: List[Dict[str, Any]] = []

    # ìš°ì„ ìˆœìœ„: reportNo ê³„ì—´
    for k in ["reportNo", "reportSn", "rptNo", "reportId", "id", "seq"]:
        if k in id_fields:
            param_candidates.append({k: id_fields[k]})

    # ë³µí•© íŒŒë¼ë¯¸í„° ì¼€ì´ìŠ¤ ëŒ€ë¹„: apbaId + rootNo ë“±ì„ ê°™ì´ ë„£ëŠ” ê²½ìš°
    # (ëª©ë¡ itemì— ë“¤ì–´ìˆë‹¤ë©´ í•¨ê»˜)
    extra_keys = ["apbaId", "reportFormRootNo", "reportRootNo", "reportFormNo"]
    extras = {k: item.get(k) for k in extra_keys if item.get(k)}
    if extras:
        for base in list(param_candidates):
            merged = dict(extras)
            merged.update(base)
            param_candidates.append(merged)

    for endpoint in DETAIL_ENDPOINT_CANDIDATES:
        for params in param_candidates:
            try:
                resp = safe_get(endpoint, params=params)
                if not is_json_response(resp):
                    continue
                dj = resp.json()
                pdf = extract_pdf_from_detail_json(dj)
                if pdf:
                    return pdf
            except Exception:
                continue
    return None

def extract_pdf_links_from_detail_html(detail_url: str) -> List[str]:
    """
    ìƒì„¸ HTMLì—ì„œ PDF/fileDown ë§í¬ë¥¼ íŒŒì‹±(2ì°¨ ì•ˆì „ì¥ì¹˜)
    """
    resp = safe_get(detail_url)
    html = resp.text
    soup = BeautifulSoup(html, "lxml")

    links: List[str] = []
    for a in soup.select("a[href]"):
        href = a.get("href", "")
        low = href.lower()
        if ".pdf" in low or "filedown" in low or "download" in low:
            links.append(urljoin(BASE, href) if href.startswith("/") else href)

    # JSì— ìˆ¨ê²¨ì§„ fileDown ê²½ë¡œê°€ ìˆëŠ” ì¼€ì´ìŠ¤ ëŒ€ë¹„: ì •ê·œì‹ìœ¼ë¡œë„ 1íšŒ ìŠ¤ìº”
    for m in re.findall(r'(https?://[^\s"\']+)', html):
        if ".pdf" in m.lower() or "filedown" in m.lower():
            links.append(m)

    # ì¤‘ë³µ ì œê±°
    deduped = list(dict.fromkeys(links))
    return deduped

def pick_best_pdf_link(links: List[str]) -> Optional[str]:
    if not links:
        return None
    # ê°€ì¥ ê·¸ëŸ´ë“¯í•œ ê²ƒ ìš°ì„ ìˆœìœ„
    for l in links:
        if ".pdf" in l.lower():
            return l
    return links[0]

# ======================================================
# PDF text extraction
# ======================================================
def download_pdf_bytes(url: str, timeout: int = 25) -> bytes:
    r = requests.get(url, headers=HEADERS, timeout=timeout)
    r.raise_for_status()
    return r.content

def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    # 1) pdfplumber
    try:
        with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:
            pages = [p.extract_text() or "" for p in pdf.pages]
        text = "\n".join(pages).strip()
        if text:
            return text
    except Exception:
        pass

    # 2) pypdf fallback
    reader = PdfReader(BytesIO(pdf_bytes))
    pages = [p.extract_text() or "" for p in reader.pages]
    return "\n".join(pages).strip()

def chunk_text(text: str, max_chars: int = 6000, overlap: int = 400) -> List[str]:
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + max_chars, n)
        chunks.append(text[start:end])
        start = end - overlap if end < n else end
    return chunks

# ======================================================
# OpenAI summarization (new SDK)
# ======================================================
def get_openai_client() -> OpenAI:
    key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not key:
        raise RuntimeError("OPENAI_API_KEY not found in secrets/env")
    return OpenAI(api_key=key)

def summarize_kwater_standard_a(client: OpenAI, model: str, text: str) -> str:
    partial = []
    for chunk in chunk_text(text):
        r = client.responses.create(
            model=model,
            input=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": chunk},
            ],
        )
        partial.append(r.output_text.strip())

    if len(partial) == 1:
        return partial[0]

    combined = "\n\n".join(partial)
    r = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": combined},
        ],
    )
    return r.output_text.strip()

# ======================================================
# Streamlit UI
# ======================================================
st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ’§", layout="wide")
st.title(APP_TITLE)

with st.sidebar:
    st.header("ALIO ê²€ìƒ‰ ì„¤ì • (Bì•ˆ ìë™ëŒ€ì‘)")
    apba_id = st.text_input("ê¸°ê´€ ì½”ë“œ (apbaId)", value="C0221")
    report_root = st.text_input("ë³´ê³ ì„œ ìœ í˜• ì½”ë“œ (reportFormRootNo)", value="B1040")
    model = st.selectbox("ëª¨ë¸", ["gpt-4o-mini", "gpt-4o"], index=0)
    page_size = st.slider("í˜ì´ì§€ í¬ê¸°", 10, 50, 30, 5)
    st.divider()
    st.caption("â€» Bì•ˆì€ ì—¬ëŸ¬ ë‚´ë¶€ API í›„ë³´ë¥¼ ìë™ìœ¼ë¡œ ì‹œë„í•´ ë™ì‘ ì¡°í•©ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.")

# session state
if "probe" not in st.session_state:
    st.session_state.probe = None
if "candidates" not in st.session_state:
    st.session_state.candidates = []
if "last_debug" not in st.session_state:
    st.session_state.last_debug = {}

col1, col2 = st.columns([1, 1])
with col1:
    if st.button("1) ëª©ë¡ API ìë™ íƒìƒ‰ + ëª©ë¡ ì¡°íšŒ", type="primary"):
        try:
            probe = probe_report_list_api(apba_id, report_root)
            st.session_state.probe = probe

            list_json = get_list_with_probe(probe, page=1, page_size=page_size)
            candidates = normalize_candidates(list_json)
            st.session_state.candidates = candidates

            st.session_state.last_debug = {
                "chosen_endpoint": probe.endpoint,
                "method": probe.method,
                "params": probe.params,
                "list_key": probe.list_key,
                "total_key": probe.total_key,
            }

            if not candidates:
                st.warning("ëª©ë¡ì€ ì‘ë‹µí–ˆì§€ë§Œ í•­ëª©ì„ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ìŠ¤í‚¤ë§ˆê°€ ë§¤ìš° íŠ¹ì´í•œ ì¼€ì´ìŠ¤)")
            else:
                st.success(f"ì¡°íšŒ ì„±ê³µ: {len(candidates)}ê±´")
        except Exception as e:
            st.error(f"ì¡°íšŒ ì‹¤íŒ¨: {e}")

with col2:
    if st.session_state.last_debug:
        with st.expander("ë””ë²„ê·¸: ìë™ ì„ íƒëœ ì‹¤ì œ ë™ì‘ ê°’(operating values)"):
            st.json(st.session_state.last_debug)

st.divider()
st.subheader("2) ë³´ê³ ì„œ ì„ íƒ â†’ PDF ì¶”ì¶œ(ìƒì„¸ JSON í”„ë¡œë¹™ â†’ ì‹¤íŒ¨ ì‹œ HTML íŒŒì‹±) â†’ ìš”ì•½")

if st.session_state.candidates:
    options = list(range(len(st.session_state.candidates)))
    idx = st.selectbox(
        "ë³´ê³ ì„œ ì„ íƒ",
        options,
        format_func=lambda i: f"{st.session_state.candidates[i].title} ({st.session_state.candidates[i].date}) {st.session_state.candidates[i].org}",
    )

    cand: ReportCandidate = st.session_state.candidates[idx]

    with st.expander("ì„ íƒ í•­ëª© ì›ë³¸(raw) ë³´ê¸°"):
        st.json(cand.raw)

    if st.button("PDF ì¶”ì¶œ + K-water í‘œì¤€ A ìš”ì•½"):
        try:
            # 1) ìƒì„¸ JSON APIë¡œ PDF ì‹œë„
            pdf_url = probe_detail_api_for_pdf(cand.raw)

            # 2) ì‹¤íŒ¨í•˜ë©´ ìƒì„¸ HTML íŒŒì‹±
            if not pdf_url:
                if not cand.detail_url:
                    raise RuntimeError("ìƒì„¸ URLì´ ì—†ì–´ HTML íŒŒì‹±ë„ ë¶ˆê°€í•©ë‹ˆë‹¤. (ëª©ë¡ JSONì— detailUrlì´ ì—†ìŒ)")
                links = extract_pdf_links_from_detail_html(cand.detail_url)
                pdf_url = pick_best_pdf_link(links)

            if not pdf_url:
                raise RuntimeError("PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ìƒì„¸ JSON/HTML ëª¨ë‘ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨)")

            st.info(f"PDF URL: {pdf_url}")

            pdf_bytes = download_pdf_bytes(pdf_url)
            text = extract_text_from_pdf(pdf_bytes).strip()
            if not text:
                st.warning("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ìŠ¤ìº”ë³¸ ê°€ëŠ¥ì„±)")
                st.stop()

            client = get_openai_client()
            with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                summary = summarize_kwater_standard_a(client, model, text)

            st.markdown(summary)

            with st.expander("ì›ë¬¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
                st.write(text[:1200])

        except Exception as e:
            st.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
else:
    st.info("ë¨¼ì € 'ëª©ë¡ API ìë™ íƒìƒ‰ + ëª©ë¡ ì¡°íšŒ'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
