# =========================
# Imports
# =========================
import os
from dataclasses import dataclass
from io import BytesIO
from typing import List, Optional
from urllib.parse import urljoin, urlparse

import pdfplumber
import requests
import streamlit as st
from bs4 import BeautifulSoup
from openai import OpenAI
from pypdf import PdfReader

# =========================
# App Configuration
# =========================
APP_TITLE = "K-water ë³´ê³ ì„œ ìš”ì•½ ì—ì´ì „íŠ¸"

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ìˆ˜ìì› ë° ê³µê³µ ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
    "ì œê³µëœ ë³´ê³ ì„œì˜ í•µì‹¬ ë‚´ìš©, ì—°êµ¬ ëª©ì , ì£¼ìš” ê²°ê³¼, ì •ì±…ì  ì‹œì‚¬ì ì„ "
    "Markdown í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."
)

USER_AGENT = (
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
)

# =========================
# Data Class
# =========================
@dataclass
class ReportSource:
    pdf_url: Optional[str]
    text: str

# =========================
# Utility Functions
# =========================
def fetch_html(url: str, timeout: int = 12) -> str:
    response = requests.get(
        url,
        headers={"User-Agent": USER_AGENT},
        timeout=timeout,
    )
    response.raise_for_status()
    return response.text


def scrape_pdf_links(page_url: str) -> List[str]:
    html = fetch_html(page_url)
    soup = BeautifulSoup(html, "lxml")
    base_url = f"{urlparse(page_url).scheme}://{urlparse(page_url).netloc}"

    links = []
    for a in soup.select("a[href]"):
        href = a.get("href", "").lower()
        if ".pdf" in href or "filedown" in href or "download" in href:
            links.append(urljoin(base_url, a["href"]))

    return list(dict.fromkeys(links))


def download_pdf(url: str, timeout: int = 20) -> bytes:
    response = requests.get(
        url,
        headers={"User-Agent": USER_AGENT},
        timeout=timeout,
    )
    response.raise_for_status()
    return response.content


def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    text = ""

    # 1ï¸âƒ£ pdfplumber (best for digital PDFs)
    try:
        with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:
            pages = [page.extract_text() or "" for page in pdf.pages]
        text = "\n".join(pages).strip()
    except Exception:
        pass

    if text:
        return text

    # 2ï¸âƒ£ pypdf fallback
    reader = PdfReader(BytesIO(pdf_bytes))
    pages = [page.extract_text() or "" for page in reader.pages]
    return "\n".join(pages).strip()


def chunk_text(text: str, max_chars: int = 6000, overlap: int = 400) -> List[str]:
    chunks = []
    start = 0
    length = len(text)

    while start < length:
        end = min(start + max_chars, length)
        chunks.append(text[start:end])
        start = end - overlap if end < length else end

    return chunks


# =========================
# OpenAI Summarization
# =========================
def summarize_text(client: OpenAI, model: str, text: str) -> str:
    chunks = chunk_text(text)
    partial_summaries = []

    for chunk in chunks:
        response = client.responses.create(
            model=model,
            input=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": chunk},
            ],
        )
        partial_summaries.append(response.output_text.strip())

    if len(partial_summaries) == 1:
        return partial_summaries[0]

    combined = "\n\n".join(partial_summaries)

    final_response = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": combined},
        ],
    )

    return final_response.output_text.strip()


def get_openai_client() -> OpenAI:
    api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY not found")
    return OpenAI(api_key=api_key)

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ’§", layout="wide")
st.title(APP_TITLE)

with st.sidebar:
    st.header("ì„¤ì •")
    model = st.selectbox(
        "ëª¨ë¸",
        ["gpt-4o-mini", "gpt-4o"],
        index=0,
    )
    preview_limit = st.slider(
        "í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ê¸¸ì´",
        min_value=300,
        max_value=2000,
        value=800,
    )

st.subheader("ë³´ê³ ì„œ ì…ë ¥")

url_input = st.text_input(
    "ALIO ê²Œì‹œê¸€ URL",
    placeholder="https://alio.go.kr/item/itemDetail.do?...",
)

uploaded_pdf = st.file_uploader(
    "PDF íŒŒì¼ ì§ì ‘ ì—…ë¡œë“œ",
    type=["pdf"],
)

# =========================
# Session State
# =========================
for key in [
    "report_text",
    "report_source",
    "summary",
    "pdf_links",
    "scrape_warning",
]:
    if key not in st.session_state:
        st.session_state[key] = "" if key != "pdf_links" else []

# =========================
# Load Report
# =========================
if st.button("ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    st.session_state.summary = ""
    st.session_state.report_text = ""
    st.session_state.report_source = None
    st.session_state.pdf_links = []
    st.session_state.scrape_warning = ""

    if not url_input and not uploaded_pdf:
        st.warning("URL ë˜ëŠ” PDF íŒŒì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        if url_input:
            try:
                st.session_state.pdf_links = scrape_pdf_links(url_input)
                if not st.session_state.pdf_links:
                    st.session_state.scrape_warning = "PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            except Exception:
                st.session_state.scrape_warning = "ìŠ¤í¬ë˜í•‘ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."

        if uploaded_pdf:
            try:
                text = extract_text_from_pdf(uploaded_pdf.read())
                st.session_state.report_text = text
                st.session_state.report_source = ReportSource("ì—…ë¡œë“œëœ íŒŒì¼", text)
            except Exception:
                st.error("PDF íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

# =========================
# PDF Selection
# =========================
if st.session_state.scrape_warning:
    st.warning(st.session_state.scrape_warning)

if st.session_state.pdf_links:
    selected_pdf = st.selectbox("ë°œê²¬ëœ PDF ë§í¬", st.session_state.pdf_links)
    if st.button("ì„ íƒí•œ PDF ë¶ˆëŸ¬ì˜¤ê¸°"):
        try:
            pdf_bytes = download_pdf(selected_pdf)
            text = extract_text_from_pdf(pdf_bytes)
            st.session_state.report_text = text
            st.session_state.report_source = ReportSource(selected_pdf, text)
        except Exception:
            st.error("PDF ë‹¤ìš´ë¡œë“œ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨")

if st.session_state.report_source:
    st.success("ë³´ê³ ì„œ ë¡œë”© ì™„ë£Œ")
    st.caption(f"ì‚¬ìš©í•œ ì†ŒìŠ¤: {st.session_state.report_source.pdf_url}")

# =========================
# Summarization
# =========================
st.divider()
st.subheader("ìš”ì•½")

if st.button("ìš”ì•½ ìƒì„±"):
    if not st.session_state.report_text:
        st.warning("ë¨¼ì € ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    else:
        try:
            client = get_openai_client()
            with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                st.session_state.summary = summarize_text(
                    client,
                    model,
                    st.session_state.report_text,
                )
        except Exception as e:
            st.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")

if st.session_state.summary:
    st.markdown(st.session_state.summary)

if st.session_state.report_text:
    with st.expander("ì›ë³¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
        st.write(st.session_state.report_text[:preview_limit])
